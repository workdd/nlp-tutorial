# Seq2Seq with Attention

## 개요
Seq2Seq with Attention은 2014년에 발표된 "Neural Machine Translation by Jointly Learning to Align and Translate" 논문에서 소개된 모델로, 기존 Seq2Seq 모델에 어텐션 메커니즘을 추가하여 성능을 크게 향상시켰습니다. 특히 기계 번역 작업에서 획기적인 성능 향상을 이루었습니다.

## 주요 특징
- 인코더-디코더 구조에 어텐션 메커니즘 추가
- 디코더가 출력을 생성할 때 입력 시퀀스의 관련 부분에 집중
- 병목 현상 해결 및 긴 시퀀스 처리 능력 향상
- 입력과 출력 간의 정렬(alignment) 학습

## 모델 구조 상세
Seq2Seq with Attention은 다음과 같은 구조로 이루어져 있습니다:

1. **인코더(Encoder)**:
   - 입력 시퀀스를 처리하는 양방향 RNN/LSTM/GRU
   - 각 입력 토큰을 순차적으로 처리
   - 모든 시점의 은닉 상태를 보존하여 디코더에 전달
   - 양방향 처리로 각 토큰의 앞뒤 문맥 정보 모두 포착

2. **어텐션 메커니즘(Attention Mechanism)**:
   - 디코더의 현재 은닉 상태와 인코더의 모든 은닉 상태 간 연관성 계산
   - 연관성 점수를 기반으로 어텐션 가중치 생성 (소프트맥스 함수 사용)
   - 가중치를 사용하여 인코더 은닉 상태의 가중합(컨텍스트 벡터) 계산
   - 컨텍스트 벡터는 현재 디코딩 시점에 중요한 입력 정보를 담고 있음

3. **디코더(Decoder)**:
   - 출력 시퀀스를 생성하는 RNN/LSTM/GRU
   - 각 시점에서 어텐션 메커니즘을 통해 컨텍스트 벡터 계산
   - 컨텍스트 벡터와 현재 은닉 상태를 결합하여 출력 생성
   - 자기회귀적(autoregressive) 방식으로 토큰 생성

## 수식 및 수학적 설명
어텐션 메커니즘의 핵심 수식은 다음과 같습니다:

1. **어텐션 점수 계산**:
   - $e_{ij} = a(s_{i-1}, h_j)$
   - $a$는 어텐션 함수로, 일반적으로 다음 중 하나를 사용:
     - 가산 어텐션(Additive): $a(s, h) = v^T \tanh(W_1 s + W_2 h)$
     - 곱 어텐션(Multiplicative): $a(s, h) = s^T W h$
     - 점 곱 어텐션(Dot-Product): $a(s, h) = s^T h$

2. **어텐션 가중치 계산**:
   - $\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x} \exp(e_{ik})}$

3. **컨텍스트 벡터 계산**:
   - $c_i = \sum_{j=1}^{T_x} \alpha_{ij} h_j$

4. **출력 계산**:
   - $s_i = f(s_{i-1}, y_{i-1}, c_i)$
   - $\hat{y}_i = g(s_i, c_i)$

여기서:
- $s_i$: 디코더의 i번째 은닉 상태
- $h_j$: 인코더의 j번째 은닉 상태
- $c_i$: i번째 시점의 컨텍스트 벡터
- $\alpha_{ij}$: i번째 출력 단어가 j번째 입력 단어에 부여하는 어텐션 가중치
- $e_{ij}$: i번째 출력 단어와 j번째 입력 단어 간의 어텐션 점수
- $\hat{y}_i$: i번째 시점의 예측 출력

## 하이퍼파라미터
Seq2Seq with Attention 모델의 주요 하이퍼파라미터는 다음과 같습니다:

- 인코더와 디코더의 은닉 상태 크기
- 인코더와 디코더의 층 수
- 임베딩 차원
- 어텐션 함수 유형 (가산, 곱, 점 곱 등)
- 드롭아웃 비율
- 학습률
- 배치 크기
- 교사 강제(Teacher Forcing) 비율

## 계산 복잡도 및 리소스 요구사항
- **시간 복잡도**:
  - 인코더: $O(T_x \times H \times D)$
  - 어텐션 계산: $O(T_y \times T_x \times H)$
  - 디코더: $O(T_y \times H \times D)$
  - 전체: $O(T_x \times T_y \times H)$
  
- **공간 복잡도**:
  - 인코더 은닉 상태 저장: $O(T_x \times H)$
  - 어텐션 가중치: $O(T_y \times T_x)$
  - 모델 파라미터: $O(H^2 + H \times D)$

- **메모리 요구사항**:
  - 긴 시퀀스 처리 시 인코더 은닉 상태 저장에 많은 메모리 필요
  - 배치 크기에 비례하여 메모리 사용량 증가

## 성능 비교
- **기존 Seq2Seq 대비**:
  - BLEU 점수: 기계 번역에서 약 20-30% 성능 향상
  - 긴 문장 처리 능력 대폭 개선
  - 학습 속도 및 수렴성 향상

- **다른 모델과의 비교**:
  - RNN 기반 모델 중 최고 성능 달성
  - 트랜스포머 등장 이전 SOTA(State-of-the-Art) 모델
  - 특히 자원이 제한된 환경에서 효율적인 성능 제공

## 장점
- 긴 시퀀스에서도 정보 손실 최소화
- 입력과 출력 간의 정렬 정보 제공
- 모델의 해석 가능성(interpretability) 향상
- 번역 품질 대폭 향상
- 어텐션 맵을 통한 시각화 가능

## 단점
- RNN의 순차적 처리로 인한 병렬화 한계
- 매우 긴 시퀀스에서 여전히 계산 비용 증가
- 어텐션 계산의 추가적인 복잡성
- 트랜스포머 모델에 비해 성능 열세

## 실용적 조언
- 양방향 LSTM/GRU를 인코더로 사용하면 성능 향상
- 가산 어텐션(Additive Attention)이 일반적으로 더 안정적인 성능 제공
- 교사 강제(Teacher Forcing) 비율을 점진적으로 줄이는 스케줄링 적용
- 빔 서치(Beam Search)를 사용하여 디코딩 품질 향상
- 어텐션 맵 시각화를 통한 모델 디버깅 및 분석

## 구현 내용
이 노트북에서는 PyTorch를 사용하여 Seq2Seq with Attention 모델을 구현하고 있습니다:

1. 번역 작업을 위한 데이터셋 준비
2. 인코더 구현:
   - 임베딩 레이어
   - 양방향 RNN/LSTM 레이어
   - 모든 은닉 상태 출력
3. 어텐션 디코더 구현:
   - 임베딩 레이어
   - 어텐션 계산 메커니즘
   - RNN/LSTM 레이어
   - 출력 레이어
4. 모델 학습 및 번역 성능 평가

## 어텐션 메커니즘의 작동 방식
1. 인코더: 입력 시퀀스의 모든 은닉 상태 출력
2. 디코더: 각 시점에서 다음을 수행
   - 현재 은닉 상태와 인코더 은닉 상태 간의 유사도 계산
   - 유사도를 기반으로 어텐션 가중치 생성
   - 가중치를 사용하여 인코더 은닉 상태의 가중합(context vector) 계산
   - 가중합과 현재 은닉 상태를 결합하여 출력 생성

## 어텐션의 장점
- 긴 시퀀스에서도 정보 손실 최소화
- 입력과 출력 간의 정렬 정보 제공
- 모델의 해석 가능성(interpretability) 향상
- 번역 품질 대폭 향상

## 응용 분야
- 기계 번역
- 텍스트 요약
- 이미지 캡셔닝
- 음성 인식
- 질의응답 시스템

## 참고 자료
- 원본 논문: [Neural Machine Translation by Jointly Learning to Align and Translate(2014)](https://arxiv.org/abs/1409.0473)
- 어텐션 메커니즘은 현대 NLP와 LLM 시스템의 핵심 구성 요소이며, 트랜스포머 모델의 기초가 되었습니다.
