# TextRNN (Recurrent Neural Network for Text)

## 개요
TextRNN은 1990년에 발표된 "Finding Structure in Time" 논문에서 소개된 RNN(Recurrent Neural Network) 개념을 텍스트 처리에 적용한 모델입니다. 시퀀스 데이터의 시간적 의존성을 모델링하는 데 효과적입니다.

## 주요 특징
- 순차적 데이터 처리에 적합한 RNN(Recurrent Neural Network) 구조
- 문장의 순차적 정보와 문맥을 포착
- 단어 간의 의존성(dependency)을 모델링
- 가변 길이 입력 처리 가능

## 모델 구조 상세
TextRNN은 다음과 같은 구조로 이루어져 있습니다:

1. **임베딩 레이어**:
   - 각 단어를 고정 크기 벡터로 변환
   - 크기: (배치 크기, 문장 길이, 임베딩 차원)
   - 사전 학습된 임베딩 사용 가능

2. **RNN 레이어**:
   - 순차적으로 단어 벡터를 처리
   - 각 시점(time step)에서 이전 상태를 고려하여 현재 상태 계산
   - 은닉 상태(hidden state)를 통해 문맥 정보 유지
   - 기본 RNN 셀 사용 (LSTM, GRU는 다른 모델에서 다룸)

3. **출력 레이어**:
   - 마지막 은닉 상태(혹은 모든 은닉 상태)를 입력으로 사용
   - 드롭아웃을 통한 정규화
   - 완전 연결 레이어를 통해 최종 분류

## 수식 표현
1. 임베딩 레이어: 
   - x_t ∈ ℝ^d (d는 임베딩 차원)

2. RNN 연산:
   - h_t = tanh(W_hx · x_t + W_hh · h_(t-1) + b_h)
   - h_t는 t 시점의 은닉 상태
   - W_hx는 입력-은닉 가중치 행렬
   - W_hh는 은닉-은닉 가중치 행렬
   - b_h는 편향(bias)

3. 출력 연산:
   - y = softmax(W_y · h_T + b_y)
   - h_T는 마지막 시점의 은닉 상태
   - W_y는 출력 가중치 행렬
   - b_y는 출력 편향

## 하이퍼파라미터
- **임베딩 차원(embedding dimension)**: 100~300
- **은닉층 크기(hidden size)**: 128~512
- **RNN 레이어 수(num_layers)**: 1~3
- **드롭아웃 비율(dropout rate)**: 0.2~0.5
- **학습률(learning rate)**: 0.001~0.01
- **배치 크기(batch size)**: 16~64
- **에포크 수(epochs)**: 10~50
- **그래디언트 클리핑(gradient clipping)**: 5.0~10.0

## 계산 복잡도 및 리소스 요구사항
- **시간 복잡도**: 
  - 순전파: O(T × H × (H + D)), T는 문장 길이, H는 은닉층 크기, D는 임베딩 차원
  - 역전파: O(T × H × (H + D))
- **공간 복잡도**: O(T × H + H × D + H × H)
- **GPU 요구사항**: 
  - 작은 모델(은닉층 크기 128): 1~2GB VRAM
  - 중간 크기 모델(은닉층 크기 256): 2~4GB VRAM
  - 대규모 모델(은닉층 크기 512, 다중 레이어): 4~8GB VRAM
- **학습 시간**: 
  - 작은 데이터셋(수천 문서): GPU에서 10~30분
  - 중간 규모 데이터셋(수만 문서): GPU에서 1~3시간
  - 대규모 데이터셋(수십만 문서): GPU에서 수 시간~하루
  - CNN 기반 모델보다 5~10배 느린 학습 속도

## 성능 비교
| 모델 | 학습 속도 | 추론 속도 | 짧은 텍스트 성능 | 긴 텍스트 성능 | 순차적 정보 포착 |
|------|---------|---------|--------------|------------|--------------|
| TextCNN | 매우 빠름 | 매우 빠름 | 우수 | 보통 | 제한적 |
| TextRNN | 느림 | 느림 | 보통 | 우수 | 우수 |
| LSTM/GRU | 매우 느림 | 느림 | 우수 | 매우 우수 | 매우 우수 |

## 장점
- 순차적 정보와 문맥을 효과적으로 포착
- 가변 길이 입력을 자연스럽게 처리
- 단어 간 의존성 모델링에 적합
- 문장의 전체적인 의미 파악에 유리
- 비교적 적은 파라미터로 구현 가능

## 단점
- 기울기 소실(vanishing gradient) 문제 발생
- 장기 의존성(long-term dependency) 포착 어려움
- 병렬 처리가 어려워 학습 속도가 느림
- 문장이 길어질수록 초기 정보 손실 발생
- LSTM이나 GRU에 비해 성능이 제한적

## 실용적 조언
- 기본 RNN보다는 LSTM이나 GRU 사용을 권장 (기울기 소실 문제 완화)
- 그래디언트 클리핑(gradient clipping)을 적용하여 학습 안정화
- 양방향(bidirectional) RNN 고려 (문맥 정보 더 효과적 포착)
- 문장 길이가 매우 긴 경우 계층적 접근 고려
- 사전 학습된 임베딩 사용 시 성능 향상
- 배치 크기를 작게 유지하여 메모리 효율성 확보
- 시퀀스 길이에 따라 패딩(padding)과 마스킹(masking) 적절히 활용

## 응용 분야
- 언어 모델링
- 텍스트 생성
- 감성 분석
- 기계 번역
- 음성 인식

## 참고 자료
- 원본 논문: [Finding Structure in Time(1990)](http://psych.colorado.edu/~kimlab/Elman1990.pdf)
- TextRNN은 LSTM, GRU 등의 발전된 RNN 구조의 기초가 되었으며, 현대 NLP와 LLM 시스템에서 중요한 역할을 합니다.
