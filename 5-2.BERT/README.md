# BERT (Bidirectional Encoder Representations from Transformers)

## 개요
BERT는 2018년 Google에서 발표한 "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" 논문에서 소개된 모델로, 트랜스포머 인코더를 기반으로 한 양방향 언어 모델입니다. 사전 학습과 미세 조정(fine-tuning)의 두 단계로 구성되며, 다양한 NLP 작업에서 획기적인 성능 향상을 이루었습니다.

## 주요 특징
- 트랜스포머 인코더 기반 구조
- 양방향 문맥 정보 활용
- 마스크드 언어 모델링(Masked Language Modeling)과 다음 문장 예측(Next Sentence Prediction) 작업으로 사전 학습
- 다양한 하위 작업에 쉽게 미세 조정 가능
- WordPiece 토큰화를 통한 미등록 단어 문제 해결

## 모델 구조 상세
BERT는 트랜스포머 인코더 레이어를 여러 층 쌓은 구조로 이루어져 있습니다:

1. **임베딩 레이어**:
   - 토큰 임베딩(Token Embedding): 각 토큰을 벡터로 변환
   - 세그먼트 임베딩(Segment Embedding): 문장 구분을 위한 임베딩
   - 위치 임베딩(Position Embedding): 토큰의 위치 정보를 제공
   - 세 임베딩의 합산값이 최종 입력 임베딩

2. **트랜스포머 인코더 레이어**:
   - 멀티 헤드 셀프 어텐션(Multi-Head Self-Attention)
   - 위치별 피드 포워드 네트워크(Position-wise Feed-Forward Network)
   - 레이어 정규화(Layer Normalization)와 잔차 연결(Residual Connection)

3. **출력 레이어**:
   - 마스크드 언어 모델링(MLM)을 위한 출력 레이어
   - 다음 문장 예측(NSP)을 위한 출력 레이어

## BERT 모델 변형
BERT는 두 가지 주요 크기로 제공됩니다:

1. **BERT-Base**:
   - 레이어 수: 12
   - 히든 크기: 768
   - 어텐션 헤드: 12
   - 파라미터 수: 약 110M

2. **BERT-Large**:
   - 레이어 수: 24
   - 히든 크기: 1024
   - 어텐션 헤드: 16
   - 파라미터 수: 약 340M

## 수식 및 수학적 설명
BERT의 핵심 수식은 다음과 같습니다:

1. **입력 임베딩**:
   - $E_{input} = E_{token} + E_{segment} + E_{position}$

2. **트랜스포머 인코더**:
   - $H_0 = E_{input}$
   - $H_l = \text{TransformerLayer}(H_{l-1})$ for $l \in [1, L]$
   - $\text{TransformerLayer}(H) = \text{FFN}(\text{LN}(H + \text{MHA}(H)))$
   - $\text{MHA}(H) = \text{MultiHeadAttention}(H, H, H)$

3. **마스크드 언어 모델링(MLM)**:
   - $P(w_i | w_{\setminus i}) = \text{softmax}(H_L W^T)$
   - $w_{\setminus i}$는 $i$번째 토큰이 마스킹된 시퀀스

4. **다음 문장 예측(NSP)**:
   - $P(\text{IsNext} | C, S_1, S_2) = \text{sigmoid}(H_L^{[CLS]} W_{NSP})$
   - $H_L^{[CLS]}$는 [CLS] 토큰의 최종 은닉 상태

## 하이퍼파라미터
BERT 모델의 주요 하이퍼파라미터는 다음과 같습니다:

- 모델 크기(Base/Large)
- 최대 시퀀스 길이: 일반적으로 512
- 마스킹 비율: 15%
- 배치 크기: 256 또는 512
- 학습률: 1e-4
- 드롭아웃 비율: 0.1
- 활성화 함수: GELU
- 옵티마이저: Adam(β1=0.9, β2=0.999)
- 워밍업 스텝: 10,000

## 계산 복잡도 및 리소스 요구사항
- **시간 복잡도**:
  - 셀프 어텐션: $O(n^2 \times d)$, $n$은 시퀀스 길이, $d$는 모델 차원
  - 전체 모델: $O(L \times n^2 \times d)$, $L$은 레이어 수
  
- **공간 복잡도**:
  - 모델 파라미터: BERT-Base 약 110M, BERT-Large 약 340M
  - 어텐션 점수 행렬: $O(n^2)$
  
- **메모리 요구사항**:
  - 배치 크기와 시퀀스 길이에 비례하여 메모리 사용량 증가
  - 미세 조정 시 더 적은 메모리 요구(그래디언트 축적 기법 사용 가능)

## 사전 학습 데이터
BERT는 다음 데이터셋으로 사전 학습되었습니다:
- BooksCorpus(800M 단어)
- 영어 위키피디아(2,500M 단어)
- 총 약 33억 단어의 텍스트 코퍼스

## 성능 비교
- **기존 단방향 모델(GPT) 대비**:
  - GLUE 벤치마크에서 평균 4.5% 성능 향상
  - SQuAD 질의응답에서 F1 점수 5.1% 향상
  
- **ELMo 대비**:
  - 문맥화된 임베딩의 품질 향상
  - 다운스트림 작업에서 일관되게 우수한 성능
  
- **미세 조정 vs 특성 기반 접근법**:
  - 미세 조정 방식이 대부분의 작업에서 더 우수한 성능
  - 특히 복잡한 작업에서 성능 차이 두드러짐

## 구현 내용
이 노트북에서는 PyTorch를 사용하여 간소화된 BERT 모델을 구현하고 있습니다:

1. 마스크드 언어 모델링과 다음 문장 예측을 위한 데이터셋 준비
2. BERT 모델 구현:
   - 임베딩 레이어(토큰, 세그먼트, 위치 임베딩)
   - 트랜스포머 인코더 레이어
   - 마스크드 언어 모델링을 위한 출력 레이어
   - 다음 문장 예측을 위한 출력 레이어
3. 모델 학습 및 성능 평가

## BERT의 사전 학습 작업
1. **마스크드 언어 모델링(MLM)**:
   - 입력 토큰의 15%를 무작위로 마스킹
   - 마스킹된 토큰을 예측하도록 학습
   - 이를 통해 양방향 문맥 정보 학습
   
2. **다음 문장 예측(NSP)**:
   - 두 문장이 연속적인지 여부를 예측
   - 문서 수준의 문맥 이해 능력 향상

## 미세 조정 방법
BERT는 다양한 NLP 작업에 쉽게 미세 조정할 수 있습니다:

1. **문장 분류**:
   - [CLS] 토큰의 최종 은닉 상태에 분류 레이어 추가
   
2. **토큰 분류(시퀀스 태깅)**:
   - 각 토큰의 최종 은닉 상태에 분류 레이어 추가
   
3. **질의응답**:
   - 시작 위치와 끝 위치를 예측하는 두 개의 분류 레이어 추가
   
4. **문장 쌍 분류**:
   - 두 문장을 [SEP] 토큰으로 구분하여 입력
   - [CLS] 토큰의 최종 은닉 상태에 분류 레이어 추가

## 장점
- 양방향 문맥 정보를 활용한 풍부한 언어 표현 학습
- 전이 학습(Transfer Learning)을 통한 효율적인 모델 활용
- 다양한 NLP 작업에서 우수한 성능
- 적은 양의 라벨링된 데이터로도 효과적인 미세 조정 가능
- 단일 모델로 다양한 작업 수행 가능
- 사전 학습된 모델을 공개하여 접근성 향상

## 단점
- 계산 비용이 높고 학습에 많은 리소스 필요
- 최대 시퀀스 길이(512 토큰)의 제한
- 자기회귀적(autoregressive) 생성이 어려움
- 사전 학습 데이터에 편향이 있을 경우 모델에 반영
- 미세 조정 시 과적합 위험

## 실용적 조언
- 미세 조정 시 낮은 학습률(2e-5 ~ 5e-5) 사용
- 배치 크기와 학습률의 적절한 조합 찾기
- 그래디언트 축적(Gradient Accumulation)을 통한 메모리 효율성 향상
- 학습률 스케줄링 및 워밍업 적용
- 데이터 증강을 통한 미세 조정 성능 향상
- 도메인 특화 사전 학습을 통한 성능 개선

## BERT의 응용 분야
- 질의응답
- 감성 분석
- 개체명 인식
- 자연어 추론
- 문서 분류
- 텍스트 요약
- 기계 번역

## 시각화 및 해석
BERT의 어텐션 가중치를 시각화하면 모델이 어떤 토큰 간의 관계에 집중하는지 확인할 수 있습니다. 특히 특정 언어 현상(예: 대명사 해소, 구문 관계)을 포착하는 어텐션 헤드를 발견할 수 있으며, 이는 모델의 언어 이해 능력을 해석하는 데 도움이 됩니다.

## BERT의 발전과 영향
BERT는 NLP 분야에 혁명적인 변화를 가져왔으며, 이후 다양한 변형 모델이 등장했습니다:

- **RoBERTa**: 더 많은 데이터와 최적화된 학습 방법으로 성능 향상
- **ALBERT**: 파라미터 공유를 통한 효율적인 모델 구조
- **DistilBERT**: 지식 증류를 통한 경량화 모델
- **ELECTRA**: 대체 토큰 탐지 방식의 효율적인 사전 학습
- **SpanBERT**: 연속된 스팬 마스킹을 통한 성능 향상

또한 GPT, T5 등 다른 트랜스포머 기반 모델의 발전에도 큰 영향을 미쳤으며, 현대 LLM의 기초가 되었습니다.

## 향후 연구 방향
- 더 효율적인 사전 학습 방법 개발
- 더 긴 시퀀스를 처리할 수 있는 모델 구조 연구
- 다국어 및 다중 모달 학습 확장
- 모델 압축 및 경량화 기법 개발
- 해석 가능성 및 편향 완화 연구

## 참고 자료
- 원본 논문: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(2018)](https://arxiv.org/abs/1810.04805)
- Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.
- BERT는 현대 NLP와 LLM 시스템의 중요한 이정표로, 사전 학습-미세 조정 패러다임을 확립하였습니다.

## 결론
BERT는 양방향 문맥 정보를 활용한 사전 학습 모델로, NLP 분야에 혁신적인 발전을 가져왔습니다. 마스크드 언어 모델링과 다음 문장 예측 작업을 통해 풍부한 언어 표현을 학습하고, 이를 다양한 하위 작업에 미세 조정하는 방식은 현대 NLP의 표준이 되었습니다. BERT의 성공은 GPT, T5 등 더 발전된 모델의 기초가 되었으며, 사전 학습-미세 조정 패러다임은 현대 언어 모델의 핵심 접근법으로 자리 잡았습니다.
