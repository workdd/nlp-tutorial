# Bi-LSTM with Attention

## 개요
Bi-LSTM with Attention은 양방향 LSTM에 어텐션 메커니즘을 결합한 모델로, 시퀀스의 양방향 문맥을 포착하면서 중요한 부분에 집중할 수 있는 능력을 갖추고 있습니다. 특히 감성 분석과 같은 텍스트 분류 작업에서 우수한 성능을 보입니다.

## 주요 특징
- 양방향 LSTM을 통한 문맥 정보 포착
- 어텐션 메커니즘을 통한 중요 정보 강조
- 문장 내 중요 단어에 가중치 부여
- 분류 작업에 최적화된 구조

## 구현 내용
이 노트북에서는 PyTorch를 사용하여 Bi-LSTM with Attention 모델을 구현하고 있습니다:

1. 이진 감성 분류를 위한 데이터셋 준비
2. 모델 구현:
   - 임베딩 레이어
   - 양방향 LSTM 레이어
   - 셀프 어텐션(Self-Attention) 메커니즘
   - 어텐션 가중치를 통한 문맥 벡터 생성
   - 출력 레이어
3. 모델 학습 및 감성 분류 성능 평가

## 어텐션 메커니즘의 작동 방식
1. 양방향 LSTM을 통해 각 단어의 은닉 상태 생성
2. 은닉 상태에 대한 어텐션 점수 계산
3. 소프트맥스를 통해 어텐션 가중치 생성
4. 가중치를 사용하여 은닉 상태의 가중합 계산
5. 최종 문맥 벡터를 사용하여 분류 수행

## Bi-LSTM with Attention의 장점
- 문장 내 중요 단어 식별 능력
- 긴 문장에서도 효과적인 정보 포착
- 모델의 결정에 대한 해석 가능성 제공
- 분류 성능 향상

## 응용 분야
- 감성 분석
- 문서 분류
- 의도 분류
- 관계 추출
- 질의응답 시스템

## 시각화 및 해석
Bi-LSTM with Attention 모델의 주요 장점 중 하나는 어텐션 가중치를 시각화하여 모델이 어떤 단어에 집중하는지 확인할 수 있다는 점입니다. 이를 통해 모델의 결정 과정을 해석할 수 있으며, 특히 감성 분석에서는 어떤 단어가 긍정/부정 감성에 기여하는지 파악할 수 있습니다.

## 참고 자료
- Bi-LSTM with Attention은 현대 NLP와 LLM 시스템에서 중요한 구성 요소이며, 특히 분류 작업에서 널리 사용됩니다.
- 이 모델은 트랜스포머 모델이 등장하기 전까지 텍스트 분류의 최첨단 모델로 사용되었습니다.
