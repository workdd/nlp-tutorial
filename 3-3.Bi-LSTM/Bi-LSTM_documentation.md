# Bi-LSTM (Bidirectional Long Short-Term Memory)

## 개요
Bi-LSTM은 LSTM의 확장 버전으로, 시퀀스를 양방향(정방향과 역방향)으로 처리하여 더 풍부한 문맥 정보를 포착하는 모델입니다. 이를 통해 각 시점에서 과거와 미래의 정보를 모두 활용할 수 있습니다.

## 주요 특징
- 양방향(Bidirectional) 정보 흐름을 통한 문맥 이해 향상
- 순방향(Forward)과 역방향(Backward) LSTM 레이어 결합
- 과거와 미래 정보를 모두 활용한 예측
- 시퀀스 레이블링 작업에 특히 효과적

## 모델 구조 상세
Bi-LSTM은 다음과 같은 구조로 이루어져 있습니다:

1. **임베딩 레이어**:
   - 각 단어를 고정 크기 벡터로 변환
   - 크기: (배치 크기, 문장 길이, 임베딩 차원)
   - 사전 학습된 임베딩 사용 가능

2. **순방향 LSTM 레이어**:
   - 입력 시퀀스를 처음부터 끝까지 처리
   - 각 시점에서 과거 정보를 활용
   - 출력: h⃗_t (t 시점의 순방향 은닉 상태)

3. **역방향 LSTM 레이어**:
   - 입력 시퀀스를 끝에서 처음까지 처리
   - 각 시점에서 미래 정보를 활용
   - 출력: h⃐_t (t 시점의 역방향 은닉 상태)

4. **결합 레이어**:
   - 각 시점에서 순방향과 역방향 은닉 상태를 결합
   - 결합 방식: 연결(concatenation), 합(sum), 평균(average) 등
   - 일반적으로 연결 방식 사용: h_t = [h⃗_t; h⃐_t]

5. **출력 레이어**:
   - 결합된 은닉 상태를 입력으로 사용
   - 드롭아웃을 통한 정규화
   - 완전 연결 레이어를 통해 최종 예측

## 수식 표현
Bi-LSTM의 전체 수식은 다음과 같습니다:

1. 순방향 LSTM:
   - h⃗_t = LSTM_forward(x_t, h⃗_(t-1), C⃗_(t-1))

2. 역방향 LSTM:
   - h⃐_t = LSTM_backward(x_t, h⃐_(t+1), C⃐_(t+1))

3. 결합된 은닉 상태:
   - h_t = [h⃗_t; h⃐_t]

4. 최종 출력:
   - y_t = softmax(W_y · h_t + b_y)

여기서 LSTM_forward와 LSTM_backward는 일반적인 LSTM 연산을 수행합니다.

## 하이퍼파라미터
- **임베딩 차원(embedding dimension)**: 100~300
- **은닉층 크기(hidden size)**: 128~1024 (각 방향별)
- **LSTM 레이어 수(num_layers)**: 1~3 (각 방향별)
- **드롭아웃 비율(dropout rate)**: 0.2~0.5
- **학습률(learning rate)**: 0.0005~0.002
- **배치 크기(batch size)**: 16~128
- **에포크 수(epochs)**: 10~100
- **그래디언트 클리핑(gradient clipping)**: 1.0~5.0
- **결합 방식**: 연결(concatenation), 합(sum), 평균(average) 등

## 계산 복잡도 및 리소스 요구사항
- **시간 복잡도**: 
  - 순전파: O(8 × T × H × (H + D)), T는 문장 길이, H는 은닉층 크기, D는 임베딩 차원
  - 역전파: O(8 × T × H × (H + D))
- **공간 복잡도**: O(T × H × 2 + 8 × H × (H + D) + 8 × H)
- **GPU 요구사항**: 
  - 작은 모델(은닉층 크기 128): 3~6GB VRAM
  - 중간 크기 모델(은닉층 크기 512): 6~12GB VRAM
  - 대규모 모델(은닉층 크기 1024, 다중 레이어): 12~24GB VRAM
- **학습 시간**: 
  - 작은 데이터셋(수천 문서): GPU에서 1~2시간
  - 중간 규모 데이터셋(수만 문서): GPU에서 4~12시간
  - 대규모 데이터셋(수십만 문서): GPU에서 1~3일
  - 단방향 LSTM보다 약 2배 느린 학습 속도

## 성능 비교
| 모델 | 학습 속도 | 추론 속도 | 문맥 이해 | 시퀀스 레이블링 | 메모리 사용량 |
|------|---------|---------|---------|--------------|------------|
| TextLSTM | 느림 | 느림 | 보통 | 보통 | 높음 |
| Bi-LSTM | 매우 느림 | 느림 | 우수 | 매우 우수 | 매우 높음 |
| Transformer | 빠름 | 빠름 | 매우 우수 | 우수 | 매우 높음 |
| BERT | 매우 느림 | 느림 | 매우 우수 | 매우 우수 | 극도로 높음 |

## 장점
- 양방향 문맥 정보를 활용하여 더 정확한 예측
- 시퀀스 레이블링 작업(개체명 인식, 품사 태깅 등)에 탁월한 성능
- 단방향 LSTM보다 더 풍부한 표현 학습
- 문장 내 모호성 해소에 효과적
- 다양한 NLP 태스크에 적용 가능

## 단점
- 계산 복잡도가 매우 높아 학습 및 추론 속도가 느림
- 메모리 요구량이 단방향 LSTM의 약 2배
- 실시간 처리에 부적합 (전체 시퀀스를 알아야 함)
- 매우 긴 시퀀스에서는 여전히 성능 제한
- 병렬 처리의 어려움

## 실용적 조언
- CRF(Conditional Random Field) 레이어와 결합 시 시퀀스 레이블링 성능 향상
- 그래디언트 클리핑과 배치 정규화를 적용하여 학습 안정화
- 순방향과 역방향 LSTM의 은닉층 크기를 동일하게 설정
- 드롭아웃을 LSTM 레이어 사이와 출력 레이어 전에 적용
- 사전 학습된 임베딩 사용 시 성능 향상
- 시퀀스 길이가 매우 길 경우 계층적 접근 고려
- 추론 시간이 중요한 경우 GRU 기반 Bi-RNN 고려
- 어텐션 메커니즘과 결합 시 더 좋은 성능 (Bi-LSTM with Attention)

## 응용 분야
- 개체명 인식(Named Entity Recognition)
- 품사 태깅(POS Tagging)
- 감성 분석
- 기계 번역
- 질의응답 시스템
- 문장 완성

## 참고 자료
- Bi-LSTM은 현대 NLP와 LLM 시스템에서 중요한 구성 요소로, 특히 시퀀스 라벨링 작업에서 널리 사용됩니다.
- 트랜스포머 모델이 등장하기 전까지 양방향 문맥 모델링의 표준으로 사용되었습니다.
