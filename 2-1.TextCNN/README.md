# TextCNN (Convolutional Neural Network for Text)

## 개요
TextCNN은 2014년에 발표된 "Convolutional Neural Networks for Sentence Classification" 논문에서 소개된 모델로, 컴퓨터 비전에서 성공적으로 사용된 CNN(Convolutional Neural Network)을 텍스트 분류에 적용한 모델입니다.

## 주요 특징
- 1D 컨볼루션을 사용하여 텍스트의 지역적 패턴 포착
- 다양한 크기의 필터를 통해 n-gram 효과 구현
- 맥스 풀링(Max Pooling)을 통한 중요 특성 추출
- 이미지 처리에 사용되는 CNN의 아이디어를 텍스트에 적용

## 모델 구조 상세
TextCNN은 다음과 같은 구조로 이루어져 있습니다:

1. **임베딩 레이어**:
   - 각 단어를 고정 크기 벡터로 변환
   - 크기: (배치 크기, 문장 길이, 임베딩 차원)
   - 사전 학습된 임베딩(Word2Vec, GloVe 등) 사용 가능

2. **컨볼루션 레이어**:
   - 여러 크기의 필터(일반적으로 3, 4, 5-gram에 해당)를 병렬로 적용
   - 각 필터는 n-gram 패턴을 포착
   - 1D 컨볼루션 연산: Conv1d(임베딩 차원, 필터 수, 커널 크기)
   - 활성화 함수로 ReLU 사용

3. **맥스 풀링 레이어**:
   - 각 필터에 대해 가장 중요한 특성만 추출
   - 가변 길이 입력을 고정 크기 출력으로 변환
   - 문장 길이에 관계없이 일관된 출력 생성

4. **완전 연결 레이어**:
   - 모든 필터의 맥스 풀링 결과를 연결(concatenate)
   - 드롭아웃을 통한 정규화
   - 최종 분류를 위한 선형 변환

## 수식 표현
1. 임베딩 레이어: 
   - x_i ∈ ℝ^k (k는 임베딩 차원)
   - X ∈ ℝ^(n×k) (n은 문장 길이)

2. 컨볼루션 연산:
   - c_i = f(w · x_(i:i+h-1) + b)
   - w는 필터, h는 필터 크기, f는 ReLU 함수

3. 맥스 풀링:
   - ĉ = max(c_1, c_2, ..., c_(n-h+1))

4. 최종 특성 벡터:
   - z = [ĉ_1, ĉ_2, ..., ĉ_m] (m은 총 필터 수)

## 하이퍼파라미터
- **임베딩 차원(embedding dimension)**: 100~300
- **필터 크기(filter sizes)**: 일반적으로 [3, 4, 5] 또는 [2, 3, 4, 5]
- **필터 수(num_filters)**: 각 크기별 50~200개
- **드롭아웃 비율(dropout rate)**: 0.3~0.5
- **학습률(learning rate)**: 0.001~0.01
- **배치 크기(batch size)**: 32~128
- **에포크 수(epochs)**: 10~50

## 계산 복잡도 및 리소스 요구사항
- **시간 복잡도**: 
  - 순전파: O(N × E × F × K), N은 문장 길이, E는 임베딩 차원, F는 필터 수, K는 커널 크기
  - 역전파: O(N × E × F × K)
- **공간 복잡도**: O(N × E + F × K × E + F)
- **GPU 요구사항**: 
  - 작은 모델(임베딩 차원 100, 필터 수 100): 1~2GB VRAM
  - 중간 크기 모델(임베딩 차원 300, 필터 수 200): 2~4GB VRAM
  - 대규모 데이터셋에서도 상대적으로 적은 메모리 요구
- **학습 시간**: 
  - 작은 데이터셋(수천 문서): GPU에서 수 분 이내
  - 중간 규모 데이터셋(수만 문서): GPU에서 10~30분
  - 대규모 데이터셋(수십만 문서): GPU에서 수 시간
  - RNN 기반 모델보다 5~10배 빠른 학습 속도

## 성능 비교
| 모델 | 학습 속도 | 추론 속도 | 짧은 텍스트 성능 | 긴 텍스트 성능 | 메모리 효율성 |
|------|---------|---------|--------------|------------|------------|
| TextCNN | 매우 빠름 | 매우 빠름 | 우수 | 보통 | 우수 |
| RNN/LSTM | 느림 | 느림 | 보통 | 우수 | 보통 |
| Transformer | 보통 | 빠름 | 우수 | 우수 | 낮음 |

## 장점
- 병렬 처리가 가능하여 학습 및 추론 속도가 매우 빠름
- 다양한 크기의 필터로 다양한 n-gram 패턴 포착
- 상대적으로 적은 수의 파라미터로 좋은 성능 달성
- 구현이 간단하고 이해하기 쉬움
- 짧은 텍스트 분류에 특히 효과적

## 단점
- 단어 순서의 전체적인 문맥 정보 포착 제한적
- 장거리 의존성(long-range dependency) 모델링 어려움
- 문장 길이가 매우 길 경우 정보 손실 발생
- 맥스 풀링으로 인해 일부 중요한 정보 손실 가능성
- 사전 학습된 임베딩의 품질에 성능이 크게 의존

## 실용적 조언
- 감성 분석, 주제 분류 등 짧은 텍스트 분류 작업에 우선적으로 고려
- 사전 학습된 임베딩(Word2Vec, GloVe, FastText)을 사용하면 성능 향상
- 필터 크기는 포착하고자 하는 n-gram 패턴에 맞게 설정 (일반적으로 2~5)
- 각 필터 크기별로 충분한 수의 필터 사용 (최소 100개 이상 권장)
- 과적합 방지를 위해 드롭아웃과 L2 정규화 함께 사용
- 학습 데이터가 적을 경우 데이터 증강(augmentation) 기법 고려
- 긴 문서의 경우, 문서를 여러 청크로 나누어 처리하는 계층적 접근 고려

## 응용 분야
- 감성 분석
- 스팸 탐지
- 주제 분류
- 의도 분류
- 질문 분류

## 참고 자료
- 원본 논문: [Convolutional Neural Networks for Sentence Classification(2014)](http://www.aclweb.org/anthology/D14-1181)
- TextCNN은 텍스트 분류 작업에서 기준(baseline) 모델로 자주 사용되며, 현대 NLP와 LLM 시스템의 중요한 구성 요소 중 하나입니다.
