# FastText

## 개요
FastText는 2016년 Facebook AI Research에서 발표한 "Bag of Tricks for Efficient Text Classification" 논문에서 소개된 텍스트 분류 모델입니다. Word2Vec의 확장 버전으로, 단어 수준을 넘어 하위 단어(subword) 정보를 활용합니다.

## 주요 특징
- 단어를 n-gram 문자 단위로 분해하여 표현
- 희소한(rare) 단어와 미등록 단어(OOV, Out-of-Vocabulary)에 대한 처리 개선
- 형태학적으로 풍부한 언어(한국어, 핀란드어 등)에 효과적
- 빠른 학습 속도와 효율적인 텍스트 분류 성능

## 모델 구조 상세
FastText 모델은 Word2Vec의 확장 버전으로, 다음과 같은 구조적 특징을 가집니다:

1. **문자 n-gram 표현**: 
   - 단어를 문자 수준 n-gram으로 분해 (예: "apple"의 3-gram은 "<ap", "app", "ppl", "ple", "le>")
   - 각 단어는 자체 벡터와 구성 n-gram 벡터의 합으로 표현
   - 일반적으로 3~6 길이의 n-gram 사용

2. **계층적 소프트맥스(Hierarchical Softmax)**:
   - 출력층의 계산 효율성을 위해 사용
   - 단어를 이진 트리의 리프 노드로 표현
   - 계산 복잡도를 O(|V|)에서 O(log|V|)로 감소

3. **해싱 트릭(Hashing Trick)**:
   - 메모리 효율성을 위해 n-gram을 고정 크기 해시 테이블에 매핑
   - 일반적으로 해시 크기는 수백만~수천만 범위 사용

4. **문장 분류 구조**:
   - 단어 임베딩의 평균(또는 합)을 문장 표현으로 사용
   - 선형 분류기를 통해 최종 클래스 예측

## 하이퍼파라미터
- **임베딩 차원(embedding dimension)**: 100~300 (작업에 따라 다름)
- **문자 n-gram 크기(char n-gram size)**: 3~6
- **최소 n-gram(minn)**: 일반적으로 3
- **최대 n-gram(maxn)**: 일반적으로 6
- **해시 테이블 크기(bucket)**: 100만~2000만
- **학습률(learning rate)**: 0.05~0.1 (점진적으로 감소)
- **윈도우 크기(window size)**: 5 (Word2Vec과 유사)
- **에포크 수(epochs)**: 5~50 (데이터셋 크기에 따라 다름)
- **최소 단어 빈도(min_count)**: 5 (학습에 포함할 단어의 최소 등장 횟수)

## 계산 복잡도 및 리소스 요구사항
- **시간 복잡도**: 
  - 학습: O(k × E × |V|), k는 에포크 수, E는 임베딩 차원, |V|는 어휘 크기
  - 추론: O(W × E), W는 문장의 단어 수
- **공간 복잡도**: O(|V| × E + B × E), B는 해시 버킷 크기
- **GPU 요구사항**: 
  - 중간 규모 데이터셋: 2~4GB VRAM
  - 대규모 데이터셋: 8GB+ VRAM 권장
  - CPU에서도 상대적으로 빠르게 학습 가능 (C++ 구현체 사용 시)
- **학습 시간**: 
  - 중간 규모 텍스트 분류 데이터셋(수만~수십만 문서): CPU에서 수 분~수 시간
  - 대규모 데이터셋(수백만 문서): GPU 사용 시 수 시간~하루
  - Word2Vec보다 약 2~10배 빠른 학습 속도

## 성능 비교
| 모델 | 희소 단어 처리 | OOV 처리 | 학습 속도 | 메모리 사용량 | 형태학적 언어 성능 |
|------|--------------|---------|---------|------------|-----------------|
| Word2Vec | 제한적 | 불가능 | 보통 | 보통 | 제한적 |
| GloVe | 제한적 | 불가능 | 느림 | 높음 | 제한적 |
| FastText | 우수 | 가능 | 빠름 | 높음 | 우수 |

## 장점
- 미등록 단어(OOV)에 대한 임베딩 생성 가능
- 희소한 단어에 대해서도 좋은 표현 학습
- 형태학적으로 유사한 단어 간의 관계 포착
- 학습 속도가 빠르고 병렬화 효율적
- 작은 데이터셋에서도 상대적으로 좋은 성능

## 단점
- 해시 충돌로 인한 성능 저하 가능성
- 단어 간 의미적 관계보다 형태적 유사성에 더 민감
- 메모리 사용량이 Word2Vec보다 큼 (n-gram 저장 필요)
- 문맥에 따른 단어 의미 변화 포착 불가 (Word2Vec과 동일한 한계)
- 단순한 평균 풀링으로 인한 문장 표현의 제한적 표현력

## 실용적 조언
- 한국어, 핀란드어, 터키어 등 형태학적으로 풍부한 언어에 특히 효과적
- 전문 용어나 고유명사가 많은 도메인에 적합 (의학, 법률 등)
- 텍스트 분류 작업에서는 기본 설정(default)으로 시작하여 점진적으로 조정
- 임베딩 학습과 분류기 학습을 동시에 수행하는 것이 일반적으로 더 효과적
- 공식 C++ 구현체(Facebook Research)를 사용하면 Python 구현체보다 훨씬 빠름
- 사전 학습된 FastText 임베딩(157개 언어 지원)을 활용하는 것도 좋은 선택

## 구현 내용
이 노트북에서는 PyTorch를 사용하여 FastText 모델을 구현하고 있습니다:

1. 문장 분류를 위한 데이터셋 준비
2. 단어와 n-gram 문자 단위 처리
3. FastText 모델 구현:
   - 임베딩 레이어
   - 평균 풀링(Average Pooling)
   - 분류 레이어
4. 모델 학습 및 문장 분류 성능 평가

## FastText의 장점
- 단어의 내부 구조를 고려하여 더 풍부한 표현 학습
- 미등록 단어에 대해서도 의미 있는 벡터 생성 가능
- 작은 데이터셋에서도 비교적 좋은 성능
- 계산 효율성이 높아 대규모 데이터셋에 적합

## 응용 분야
- 텍스트 분류
- 감성 분석
- 언어 식별
- 스팸 필터링
- 문서 태깅

## 참고 자료
- 원본 논문: [Bag of Tricks for Efficient Text Classification(2016)](https://arxiv.org/pdf/1607.01759.pdf)
- FastText는 효율적인 텍스트 분류와 단어 표현 학습을 위한 중요한 모델로, 현대 NLP와 LLM 시스템에서 널리 사용됩니다.
